"use strict";(self.webpackChunkzio_site=self.webpackChunkzio_site||[]).push([[51040],{15680:(e,r,t)=>{t.d(r,{xA:()=>u,yg:()=>y});var n=t(96540);function a(e,r,t){return r in e?Object.defineProperty(e,r,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[r]=t,e}function o(e,r){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);r&&(n=n.filter((function(r){return Object.getOwnPropertyDescriptor(e,r).enumerable}))),t.push.apply(t,n)}return t}function i(e){for(var r=1;r<arguments.length;r++){var t=null!=arguments[r]?arguments[r]:{};r%2?o(Object(t),!0).forEach((function(r){a(e,r,t[r])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(r){Object.defineProperty(e,r,Object.getOwnPropertyDescriptor(t,r))}))}return e}function c(e,r){if(null==e)return{};var t,n,a=function(e,r){if(null==e)return{};var t,n,a={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],r.indexOf(t)>=0||(a[t]=e[t]);return a}(e,r);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)t=o[n],r.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var l=n.createContext({}),p=function(e){var r=n.useContext(l),t=r;return e&&(t="function"==typeof e?e(r):i(i({},r),e)),t},u=function(e){var r=p(e.components);return n.createElement(l.Provider,{value:r},e.children)},s="mdxType",d={inlineCode:"code",wrapper:function(e){var r=e.children;return n.createElement(n.Fragment,{},r)}},m=n.forwardRef((function(e,r){var t=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,u=c(e,["components","mdxType","originalType","parentName"]),s=p(t),m=a,y=s["".concat(l,".").concat(m)]||s[m]||d[m]||o;return t?n.createElement(y,i(i({ref:r},u),{},{components:t})):n.createElement(y,i({ref:r},u))}));function y(e,r){var t=arguments,a=r&&r.mdxType;if("string"==typeof e||a){var o=t.length,i=new Array(o);i[0]=m;var c={};for(var l in r)hasOwnProperty.call(r,l)&&(c[l]=r[l]);c.originalType=e,c[s]="string"==typeof e?e:a,i[1]=c;for(var p=2;p<o;p++)i[p]=t[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,t)}m.displayName="MDXCreateElement"},69729:(e,r,t)=>{t.r(r),t.d(r,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>c,toc:()=>p});var n=t(58168),a=(t(96540),t(15680));const o={id:"zio-apache-parquet",title:"ZIO Apache Parquet"},i=void 0,c={unversionedId:"ecosystem/community/zio-apache-parquet",id:"ecosystem/community/zio-apache-parquet",title:"ZIO Apache Parquet",description:"ZIO Apache Parquet is a ZIO-powered Apache Parquet library.",source:"@site/docs/ecosystem/community/zio-apache-parquet.md",sourceDirName:"ecosystem/community",slug:"/ecosystem/community/zio-apache-parquet",permalink:"/ecosystem/community/zio-apache-parquet",draft:!1,editUrl:"https://github.com/zio/zio/edit/series/2.x/docs/ecosystem/community/zio-apache-parquet.md",tags:[],version:"current",frontMatter:{id:"zio-apache-parquet",title:"ZIO Apache Parquet"},sidebar:"ecosystem-sidebar",previous:{title:"ZIO AMQP",permalink:"/ecosystem/community/zio-amqp"},next:{title:"ZIO Arrow",permalink:"/ecosystem/community/zio-arrow"}},l={},p=[{value:"Introduction",id:"introduction",level:2},{value:"Features",id:"features",level:2},{value:"Installation",id:"installation",level:2},{value:"Example",id:"example",level:2}],u={toc:p},s="wrapper";function d(e){let{components:r,...t}=e;return(0,a.yg)(s,(0,n.A)({},u,t,{components:r,mdxType:"MDXLayout"}),(0,a.yg)("p",null,(0,a.yg)("a",{parentName:"p",href:"https://github.com/grouzen/zio-apache-parquet"},"ZIO Apache Parquet")," is a ZIO-powered Apache Parquet library."),(0,a.yg)("h2",{id:"introduction"},"Introduction"),(0,a.yg)("p",null,"ZIO Apache Parquet is a ZIO wrapper for ",(0,a.yg)("a",{parentName:"p",href:"https://github.com/apache/parquet-java"},"parquet-java")," and ",(0,a.yg)("a",{parentName:"p",href:"https://github.com/apache/hadoop"},"hadoop")," to easily read and write Parquet files without running a Spark cluster."),(0,a.yg)("h2",{id:"features"},"Features"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"ZIO & ZIO Streams")," \u2014 naturally integrates with ZIO ecosystem."),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"ZIO Schema")," \u2014 type-safe filter predicates and codecs derivation."),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Clean and simple API")," \u2014 easy to use, no magic.")),(0,a.yg)("h2",{id:"installation"},"Installation"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-scala"},'libraryDependencies += "me.mnedokushev" %% "zio-apache-parquet-core" % "0.1.0"\n')),(0,a.yg)("h2",{id:"example"},"Example"),(0,a.yg)("p",null,"A simple demo showcasing how to write a small chunk of data and read it applying a filter predicate."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-scala"},'//> using scala "3.5.0"\n//> using dep me.mnedokushev::zio-apache-parquet-core:0.1.0\n\nimport zio.*\nimport zio.schema.*\nimport me.mnedokushev.zio.apache.parquet.core.codec.*\nimport me.mnedokushev.zio.apache.parquet.core.hadoop.{ ParquetReader, ParquetWriter, Path }\nimport me.mnedokushev.zio.apache.parquet.core.filter.syntax.*\nimport me.mnedokushev.zio.apache.parquet.core.filter.*\n\nimport java.nio.file.Files\n\nobject Filtering extends ZIOAppDefault:\n\n  case class MyRecord(a: Int, b: String, c: Option[Long])\n\n  object MyRecord:\n    // We need to provide field names using singleton types\n    given Schema.CaseClass3.WithFields["a", "b", "c", Int, String, Option[Long], MyRecord] =\n      DeriveSchema.gen[MyRecord]\n    given SchemaEncoder[MyRecord]                                                          =\n      Derive.derive[SchemaEncoder, MyRecord](SchemaEncoderDeriver.default)\n    given ValueEncoder[MyRecord]                                                           =\n      Derive.derive[ValueEncoder, MyRecord](ValueEncoderDeriver.default)\n    given ValueDecoder[MyRecord]                                                           =\n      Derive.derive[ValueDecoder, MyRecord](ValueDecoderDeriver.default)\n    given TypeTag[MyRecord]                                                                =\n      Derive.derive[TypeTag, MyRecord](TypeTagDeriver.default)\n\n    // Define accessors to use them later in the filter predicate.\n    // You can give any names to the accessors as we demonstrate here.\n    val (id, name, age) = Filter[MyRecord].columns\n\n  val data =\n    Chunk(\n      MyRecord(1, "bob", Some(10L)),\n      MyRecord(2, "bob", Some(12L)),\n      MyRecord(3, "alice", Some(13L)),\n      MyRecord(4, "john", None)\n    )\n\n  val recordsFile = Path(Files.createTempDirectory("records")) / "records.parquet"\n\n  override def run =\n    (\n      for {\n        writer   <- ZIO.service[ParquetWriter[MyRecord]]\n        reader   <- ZIO.service[ParquetReader[MyRecord]]\n        _        <- writer.writeChunk(recordsFile, data)\n        fromFile <- reader.readChunkFiltered(\n                      recordsFile,\n                      filter(\n                        MyRecord.id > 1 `and` (\n                          MyRecord.name =!= "bob" `or`\n                            // Use .nullable syntax for optional fields.\n                            MyRecord.age.nullable > 10L\n                        )\n                      )\n                    )\n        _        <- Console.printLine(fromFile)\n      } yield ()\n    ).provide(\n      ParquetWriter.configured[MyRecord](),\n      ParquetReader.configured[MyRecord]()\n    )\n  // Outputs:\n  // Chunk(MyRecord(2,bob,Some(12)),MyRecord(3,alice,Some(13)),MyRecord(4,john,None))\n')),(0,a.yg)("p",null,"See the full docs on the project's ",(0,a.yg)("a",{parentName:"p",href:"https://mnedokushev.me/zio-apache-parquet/"},"website"),"."))}d.isMDXComponent=!0}}]);